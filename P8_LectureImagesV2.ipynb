{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 8 : Déployer un modèle dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Présentation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet s'inscrit dans le cadre du développement d'une application mobile qui permettrait aux utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n",
    "\n",
    "L'objectif de ce projet est de développer un environnement Big Data qui comprendra le preprocessing et une étape de réduction de dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fruits.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"agritech.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Banque d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données est un ensemble d'images de fruits et de labels associés :\n",
    "https://www.kaggle.com/moltean/fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images_folders.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exécution du fichier python peut de deux manières :\n",
    "- en local en spécifiant l'argument True\n",
    "- en mode AWS en spécifiant l'argument False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Présentation de Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pyspark.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les RDD sont la principale innovation apportée par Spark.\n",
    "Ils possèdent deux types de méthodes :\n",
    "- les transformations qui donnent en sortie des RDD\n",
    "- les actions qui donnent en sortie un résultat\n",
    "\n",
    "C'est au moment d'une action que les différentes transformations utilisées sont exécutées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rdd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution des calculs sur les executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un job Spark est constitué d'un ensemble d'étapes, elles-mêmes constituées d'un ensemble de tâches.\n",
    "\n",
    "Un job Spark correspond à une action sur un RDD et est composé de plusieurs étapes séparées par des shuffles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"spark_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque tâche s'éxecute sur une partition différente des données et ces partitions sont créées par les RDD.\n",
    "\n",
    "Les partitions sont réparties sur les différents executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"spark_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 2 fonctionnalités principales utilisées dans ce script sont les RDD et les udf.\n",
    "Le principe des RDD a été décrit ci-dessus. Nous utilisons également les pyspark dataFrame qui utilisent la technicité des RDD.\n",
    "\n",
    "Quant aux udf ils permettent d'ajouter une nouvelle colonne à un dataFrame, comme étant le résultat d'une fonction appliqué à une colonne existante.\n",
    "\n",
    "Le code de traitement est composé de 6 blocs distincts :\n",
    "1. Le chargement des librairies\n",
    "2. La fonction de chargement des données qui renvoie un DataFrame contenant le chemin d'accès aux données\n",
    "3. La fonction d'exctraction des catégories qui s'utilise via une udf\n",
    "4. La fonction de lecture des images qui renvoie un nouveau DataFrame avec une colonne supplémentaire correspondant aux données images\n",
    "5. La fonction de réduction dimensionelle par PCA qui renvoie un nouveau DataFrame ajouté d'une colonne correspondant aux données réduites\n",
    "6. La fonction main qui execute toutes les fonctions listées ci-dessus et qui enregistre au format parquet les résultats\n",
    "    \n",
    "Le script python est à executer en fournissant un argument True ou False selon qu'il est executé en local ou sur la plateforme AWS. Dans le cas d'une execution sur la plateforme AWS, la connexion se fait via la librairie boto3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, DataType, FloatType\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "\n",
    "\n",
    "# Fonction pour ouvrir l'image à partir de son chemin d'accès\n",
    "from PIL import Image\n",
    "\n",
    "# Librairies classiques\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "# Librairie pour se connecter au service S3 d'AWS\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lecture et chargement des fichiers images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définit le mode de chargement des noms de fichiers images lorsque le traitement est en local\n",
    "def lect_donnees_local(list_img):\n",
    "    \n",
    "    debut = datetime.datetime.now()\n",
    "    time1 = time.time()\n",
    "    print('Début traitement : ', debut)\n",
    "    \n",
    "    # initialisations\n",
    "    list_img =  []\n",
    "    chemin = '/media/sf_Public/Fruit-Images-Dataset/Training/'\n",
    "    \n",
    "    # on récupère les répertoires des catégories d'images\n",
    "    Ss_Rep = os.listdir(chemin)\n",
    "    \n",
    "    for rep in Ss_Rep:\n",
    "        lst_categ = os.listdir(chemin +\"/\" + rep)\n",
    "        for file in lst_categ:\n",
    "            list_img.append(chemin + \"/\" + rep + \"/\" + file)\n",
    "            \n",
    "    # Affichage du temps de calcul\n",
    "    fin = time.time()\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - time1))\n",
    "    \n",
    "    return(list_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datas(folder, lst_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retourne un dataFrame pyspark avec comme colonne la liste des chemins d'accès  des images du dossier folder\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lst_path =  []\n",
    "    \n",
    "    # Suivant l'argument sys.argv[1], connexion en local ou sur AWS\n",
    "    if sys.argv[1] == 'True':\n",
    "        lect_donnees_local(list_img)\n",
    "        lst_path = list_img\n",
    "    else :\n",
    "        # Connexion à l'espace de stockage S3 d'AWS\n",
    "        session = boto3.session.Session(aws_access_key_id=\"\",\n",
    "                                        aws_secret_access_key=\"\")\n",
    "        s3_client = session.client(service_name='s3', region_name=\"eu-west-3\")\n",
    "\n",
    "        prefix = 'data'\n",
    "        sub_folders = s3_client.list_objects_v2(Bucket=\"lbobucket\", Prefix=prefix)\n",
    "\n",
    "        if \"Contents\" not in sub_folders:\n",
    "            print(\"Erreur lors du chargement des images\")\n",
    "            print(\"Le dossier source n'a pas été trouvé\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        for key in sub_folders[\"Contents\"]:\n",
    "\n",
    "            file = key[\"Key\"]\n",
    "            file = file.replace(prefix + \"/\", \"\")\n",
    "            lst_path.append(folder + file)\n",
    "\n",
    "    print(\"Nombre d'images chargées :\", len(lst_path))\n",
    "    \n",
    "    # Création d'un RDD à partit de la liste des chemins d'accès aux images\n",
    "    rdd = sc.parallelize(lst_path)\n",
    "    row_rdd = rdd.map(lambda x: Row(x))\n",
    "    \n",
    "    # Création d'un dataFrame pyspark à partir d'un RDD\n",
    "    df = spark.createDataFrame(row_rdd, [\"path_img\"])\n",
    "\n",
    "    # Affichage du temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - start_time))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fonction d'extraction des catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categ(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retourne le nom du dossier dans lequel se trouve l'image,\n",
    "    qui correspond à la catégorie de fruits.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_file = path.split(\"/\")\n",
    "    categ = list_file[-2]\n",
    "    \n",
    "    return categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fonction de lecture des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(df, col_path='path_img', new_size=(20, 20)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cette fonction prend comme en entrée un dataframe pyspark avec les noms des chemins d'accès aux images, les ouvres\n",
    "    et renvoie le dataframe d'entrée avec une colonne supplémentaire qui est l'image sous forme de liste.\n",
    "    \n",
    "    Paramètres\n",
    "    df(pyspark DataFrame): contient une colonne avec le chemin d'accès aux images\n",
    "    col_path(string): nom de la colonne où récupérer le chemin d'accès aux images\n",
    "    new_size(tuple): nouvelle taille d'image\n",
    "    \"\"\"\n",
    "\n",
    "    sys.argv[1] = 'True'\n",
    "    \n",
    "    # Traitement en mode local\n",
    "    if sys.argv[1] == 'True':\n",
    "        \n",
    "        # fonction identitée :  renvoie le même chemin d'accès\n",
    "        def get_path(img_path):\n",
    "            return img_path\n",
    "\n",
    "\n",
    "    # Traitement en mode AWS\n",
    "    else:\n",
    "        \n",
    "        # Traitement spécifique pour l'accès à S3 via la librairie boto3\n",
    "        def get_path(img_path):\n",
    "            img_path = img_path.replace(\"s3://lbobucket/\", \"\")\n",
    "            s3 = boto3.resource(\"s3\", region_name='eu-west-3')\n",
    "            bucket = s3.Bucket(\"lbobucket\")\n",
    "            object = bucket.Object(img_path)\n",
    "            response = object.get()\n",
    "            file_stream = response['Body']\n",
    "            return file_stream\n",
    "        \n",
    "    # Ouvre l'image via la librairie pillow et resize l'image pour des raisons de mémoires\n",
    "    def open_img(img_path, size=new_size):\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        image = image.resize((20, 20))\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Initilisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Retourne l'image correspondante sous forme de liste pour chaque chemin d'accès d'image\n",
    "    # flatten() pour unidimensionnaliser le tableau (images couleurs)\n",
    "    # tolist() car pyspark n'accepte pas le format numpy\n",
    "    ud_f = udf(lambda img_path: np.asarray(open_img(get_path(img_path))).flatten().tolist())\n",
    "\n",
    "    df = df.withColumn('image', ud_f(col_path))\n",
    "\n",
    "    # Affiche le temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - start_time))\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fonction de réduction dimmensionnelle par PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transformation(df, n_components = 50, col_image = 'image'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applique un algorithme de PCA sur l'ensemble des images pour réduire la dimension de chaque image des données.\n",
    "    \n",
    "    Paramètres:\n",
    "    df(pyspark dataFrame): contient une colonne avec les données images\n",
    "    n_components(int): nombre de dimensions à conserver\n",
    "    col_image(string): nom de la colonne où récupérer les données images\n",
    "    \"\"\"\n",
    "\n",
    "    # Initilisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Les données images sont converties au format vecteur dense\n",
    "    ud_f = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "    df = df.withColumn('image', ud_f('image'))\n",
    "    \n",
    "    standardizer = StandardScaler(inputCol=\"image\", outputCol=\"scaledFeatures\",\n",
    "                                  withStd=True, withMean=True)\n",
    "    model_std = standardizer.fit(df)\n",
    "    df = model_std.transform(df)\n",
    "\n",
    "    # Entrainement de l'algorithme\n",
    "    pca = PCA(k=n_components, inputCol='scaledFeatures', outputCol='pcaFeatures')\n",
    "    model_pca = pca.fit(df)\n",
    "\n",
    "    # Transformation des images sur les k premières composantes\n",
    "    df = model_pca.transform(df)\n",
    "\n",
    "    df = df.filter(df.pcaFeatures.isNotNull())\n",
    "    \n",
    "    # Affiche le temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - start_time))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o207.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:847)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: /media/sf_Public/LBo/results/_temporary/0/task_202202080953442441150122921289887_0013_m_000002/part-00002-68f2ff53-1627-4ef3-b5fb-cb220e587400-c000.snappy.parquet (Aucun fichier ou dossier de ce type)\n\tat java.base/java.io.FileInputStream.open0(Native Method)\n\tat java.base/java.io.FileInputStream.open(FileInputStream.java:219)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:364)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:374)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:613)\n\tat org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:414)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:428)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:362)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:334)\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:167)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:220)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-6ffe877cbc91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'results'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/sf_Public/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sf_Public/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o207.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:847)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: /media/sf_Public/LBo/results/_temporary/0/task_202202080953442441150122921289887_0013_m_000002/part-00002-68f2ff53-1627-4ef3-b5fb-cb220e587400-c000.snappy.parquet (Aucun fichier ou dossier de ce type)\n\tat java.base/java.io.FileInputStream.open0(Native Method)\n\tat java.base/java.io.FileInputStream.open(FileInputStream.java:219)\n\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:364)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:374)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:613)\n\tat org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:414)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:428)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:362)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:334)\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:167)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:220)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "df.write.parquet(path = 'results', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fonction d'éxécution prinicpale du programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    sys.argv[1] == 'True'\n",
    "    \n",
    "    # Définis le chemin d'accès au dossier des images\n",
    "    # Chemins différents suivant si le script est executé en local ou sur AWS\n",
    "    try :\n",
    "        if sys.argv[1] == 'True':\n",
    "            folder = \"/media/sf_Public/Fruit-Images-Dataset/Training/\"\n",
    "        else :\n",
    "            folder = \"s3://lbobucket/data/\"\n",
    "    except :\n",
    "        sys.exit(0)\n",
    "    print(folder)\n",
    "\n",
    "    # Démarre la session Spark\n",
    "    try :\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        sc.setLogLevel('WARN')\n",
    "        spark = SparkSession.builder.appName(\"name\").getOrCreate()\n",
    "    except :\n",
    "        print(\"Erreur à la construction du moteur spark\")\n",
    "\n",
    "    print(\"---Liste des images---\")\n",
    "    df = load_datas(folder, lst_path)\n",
    "    df.show(5, False)\n",
    "\n",
    "    print(\"----Extraction des catégories images-----\")\n",
    "    udf_categ = udf(extract_categ, StringType())\n",
    "    df = df.withColumn(\"categ\", udf_categ('path_img'))\n",
    "\n",
    "    print(\"---Chargement des images---\")\n",
    "    df = read_images(df)\n",
    "    df.show(5)\n",
    "\n",
    "    print(\"---Réduction dimmensionnelle---\")\n",
    "    df = pca_transformation(df)\n",
    "    df.show(5)\n",
    "\n",
    "    print(\"---- Enregistrement des résultats ----\")\n",
    "    # Initilisation du temps de calcul pour l'enregistrement\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Ecrit les résultats en mode parquet\n",
    "    if sys.argv[1] == 'True':\n",
    "        df.write.parquet(path='results', mode='overwrite')\n",
    "    else :\n",
    "        df.write.parquet(path='s3://lbobucket/results/', mode='overwrite')\n",
    "    \n",
    "    # Affiche le temps de calcul de l'écriture des résultats\n",
    "    print(\"Temps d'execution : {:.2f} secondes\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
